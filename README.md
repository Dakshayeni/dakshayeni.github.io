> **Hello, I'm Dakshayeni!**  
> Data Scientist | Data Engineer | Machine Learning Enthusiast  
> Passionate about transforming data into insights and building scalable solutions.  

ðŸ”— [GitHub](https://github.com/Dakshayeni) | [LinkedIn](https://linkedin.com/in/dakshayeni) | [Email](mailto:reddydakshayeni@gmail.com)  

---

## **About Me**  

I am a **Data Scientist & Big Data Engineer** with a strong foundation in Machine Learning, Data Engineering, and Analytics. With **2+ years at JPMorgan Chase** and **ongoing research in NLP**, I have worked on **scalable ETL pipelines, cloud data architectures, and ML models**.  

ðŸ“Œ **Masterâ€™s in Data Science (Stevens Institute of Technology)** â€“ Graduation: **Dec 2024**  
ðŸ“Œ Experienced in **Big Data, Cloud Computing (AWS, GCP), Predictive Modeling, and NLP**  
ðŸ“Œ Proficient in **Python, SQL, PySpark, Tableau, and Machine Learning techniques**  

**Key Interests:**  
âœ… Data Science & AI ðŸ”¹ Big Data Engineering ðŸ”¹ Cloud Computing ðŸ”¹ Machine Learning ðŸ”¹ NLP  

---

## **Experience**  

### **ðŸ”¹ Research Assistant â€“ Stevens Institute of Technology** *(Aug 2023 â€“ Jan 2025)*  
- Developing a **Retrieval-Augmented Generation (RAG) model** to enhance text generation using external knowledge sources.  
- Implemented **advanced NLP techniques** such as embeddings, tokenization, and fine-tuning to improve model accuracy.  

### **ðŸ”¹ Data Engineer â€“ JP Morgan Chase** *(Jun 2021 â€“ Jun 2023)*  
- Developed the Integrated Data Platform processing JSON data from Kafka using batch processing, storing data in HBase and Hive for efficient retrieval and analysis, supporting scalable solutions.
- Optimized data architectures with a focus on cloud integration using AWS, ensuring seamless data processing and adherence to best practices in ETL processes migrating data to PostgreSQL.
- Achieved a 25% reduction in data processing time through efficient pipeline optimization, aiding in improved analytics capabilities.
- Collaborated with cross-functional teams ensuring data accuracy and integrity during migrations, contributing towards high-quality data deliverables.
- Implemented CI/CD best practices to optimize software deployments, enhancing the consistency and reliability of data pipelines.
- Addressed performance bottlenecks for a streamlined processing workflow, while designing schemas that support optimal data storage solutions.
- Created and managed Splunk dashboards for detailed performance analysis and developed strategic alerts monitoring log data efficiency.


### **ðŸ”¹ Intern â€“ JP Morgan Chase** *(Jan 2021 â€“ Jun 2021)*  
- Developed an **Integrated Data Platform** processing **10M+ records**, improving data access efficiency.  
- Worked with **Kafka, HBase, and Hive** for real-time data processing.
- Contributed to developing the Integrated Data Platform by processing JSON data with batch protocols, improving data retrieval through systems like HBase and Hive.
- Involved in requirement analysis with clients, delivering tailored data solutions that address complex needs efficiently and effectively.
- Successfully ensured project delivery consistency, maintaining high satisfaction rates due to keen attention to quality data processes.
 

---

## **Projects**  

### **[Book Recommendation System](https://github.com/Dakshayeni/book-recommendation)**
**Tech Stack:** Python, Scikit-learn, Data Mining, KNN  
- Built a **collaborative filtering recommendation system** using **K-Nearest Neighbors (KNN)**.  
- Improved accuracy through **data preprocessing and cosine similarity metrics**.

### **[Interest Rate Prediction using RNN and LSTM](https://github.com/Dakshayeni/Interest-Rate-Prediction-using-RNN-and-LSTM)**
- Developed a deep learning model using RNN and LSTM to predict interest rates based on historical economic data.
- Preprocessed and normalized macroeconomic indicators, including the Federal Funds Rate and unemployment rates, to improve model accuracy.
- Implemented time-series forecasting techniques to analyze temporal dependencies and long-term trends in financial data.
- Optimized model performance using hyperparameter tuning and evaluated accuracy using Mean Squared Error (MSE).
- Visualized predicted vs. actual interest rates to interpret model performance and derive economic insights.
- Demonstrated expertise in deep learning, time-series analysis, and financial forecasting using Python and TensorFlow/Keras.

### **[Image classification CNN](https://github.com/Dakshayeni/Image-classification-CNN)** 
- Developed a Convolutional Neural Network (CNN) model to classify images of cats and dogs using the Kaggle "Dogs vs. Cats" dataset.**  
- Implemented data preprocessing techniques, including dataset splitting, normalization, and augmentation, to improve model generalization.  
- Experimented with five different CNN architectures to optimize classification accuracy and reduce overfitting.  
- Applied dropout regularization and data augmentation techniques to enhance model robustness and prevent overfitting.  
- Achieved high classification accuracy by fine-tuning hyperparameters and monitoring training/validation loss curves.  


### **[Analytics in Retail Sales: A Machine Learning Approach to Enhancing Customer Experience](https://github.com/Dakshayeni/MachineLearning-Walmart-Retail-Analysis)**	
- Developed a machine learning solution to enhance predictive analytics for retail sales forecasting.
- Utilized algorithms such as Linear Regression, K-Means clustering, and Artificial Neural Networks (ANNs) to predict sales trends and customer purchase behaviors.
- Preprocessed a large dataset (~1.9M records) involving product descriptions, user transactions, and store location data from a multi-national retail chain.
- Improved the accuracy of sales predictions by fine-tuning models and applying advanced feature engineering techniques.
- Created insightful visualizations, including monthly sales patterns, customer segmentation, and geographical sales trends, helping retail managers in strategic decision-making.
- Applied clustering techniques to identify customer groups, which led to targeted marketing campaigns and optimized product bundling.


### **[Predictive Modeling of Chicago Crime Patterns](https://github.com/Dakshayeni/chicago-crime)**
 **Tech Stack:** Python, SQL, Tableau, PySpark, GCP  
- Built **crime prediction models** using **XGBoost, Decision Trees, Random Forest, Naive Bayes** (up to **97% accuracy**).
- Conducted **performance analysis (1-4 worker nodes on GCP)** to optimize execution time.  

### **[NYC Vehicle Collision Analysis](https://github.com/Dakshayeni/nyc-collision-analysis)**  
 **Tech Stack:** Python, Pandas, Tableau  
- Conducted **statistical analysis (Chi-square, ANOVA)** on **New York vehicle collisions**.  
- Developed **interactive Tableau dashboards** for accident trend visualization.    
  

---

## **Skills**  

### **Programming & Tools**  
ðŸŸ¢ Python | SQL | R | Java | SAS | PySpark | SAP  

### **Big Data & Databases**  
ðŸŸ¢ Hadoop | Apache Spark | Kafka | PostgreSQL | MySQL | NoSQL (MongoDB)  

### **Cloud & DevOps**  
ðŸŸ¢ Google Cloud (GCP) | AWS | HBase | Hive | CI/CD Pipelines  

### **Machine Learning & AI**  
ðŸŸ¢ Supervised & Unsupervised Learning | NLP | Deep Learning | Time Series Forecasting  

### **Data Visualization**  
ðŸŸ¢ Tableau | Power BI | Matplotlib | Seaborn  

---

## **Contact**  
ðŸ“§ **Email:** [reddydakshayeni@gmail.com](mailto:reddydakshayeni@gmail.com)  
ðŸ”— **LinkedIn:** [linkedin.com/in/dakshayeni](https://linkedin.com/in/dakshayeni/)  

---

