> **Hello, I'm Dakshayeni!**  
> Data Scientist | Data Engineer | Machine Learning Enthusiast  
> Passionate about transforming data into insights and building scalable solutions.  

ðŸ”— [GitHub](https://github.com/Dakshayeni) | [LinkedIn](https://www.linkedin.com/in/dakshayeni-bujunuru-84b189143/) | [Email](mailto:dakshayenibujunuru@gmail.com)  

---

## **About Me**  

Specialized in chatbot consulting and conversational AI strategy, with hands-on experience designing and deploying enterprise-grade AI assistants using Microsoft Copilot Studio, Power Virtual Agents, LangChain, and Semantic Kernel. Successfully advised clients on end-to-end chatbot architectureâ€”including use case discovery, tool selection, conversational design, API integration, and security (OAuth 2.0, JWT). Enabled automation of internal and customer-facing workflows by integrating bots with systems like Salesforce, SAP, and ServiceNow. Led chatbot initiatives that enhanced customer support, IT helpdesk, HR self-service, and procurement using natural language interfaces across Microsoft Teams and web platforms. Delivered scalable, secure, and intelligent chatbot solutions aligned with business goals and operational KPIs.

---

## **Experience**  

**Client: United Healthcare Group, Hartford, CT                                                                                                 Jan 2024 to till date
Role: Sr. AI/ML Engineer/Chatbot Consultant
Roles & Responsibilities:   **
â€¢	Tackled highly imbalanced healthcare claims and fraud datasets using sampling techniques like under-sampling and oversampling with SMOTE (Synthetic Minority Over-Sampling Technique) using Python Scikit-learn.
â€¢	Utilized PCA, t-SNE, and other feature engineering techniques to reduce high-dimensional medical data, applied feature scaling, and handled categorical attributes using one-hot encoding in Scikit-learn.
â€¢	Implemented RESTful API integrations into chatbot platforms to dynamically fetch and serve data such as order status, user profile, and ticketing history, improving user engagement and self-service capabilities.
â€¢	Used middleware (Azure Functions, Logic Apps) to build scalable connectors between chatbots and third-party systems (e.g., Salesforce, SAP, ServiceNow), enhancing business process automation.
â€¢	Developed various machine learning models such as Logistic Regression, Random Forest, and Gradient Boosting (XGBoost) with Pandas, NumPy, Seaborn, Matplotlib, and Scikit-learn in Python to predict patient readmissions and optimize healthcare costs.
â€¢	Engineered chatbot extensions to consume external REST and GraphQL APIs, dynamically parsing and presenting responses using adaptive cards for interactive, real-time feedback loops with users.
â€¢	Used OpenAPI/Swagger specifications to auto-generate connectors and enforce schema validation when Copilot Studio exchanged data with external systems.
â€¢	Implemented Agentic AI-driven chatbots capable of autonomous reasoning and tool-use orchestration across a chain of tasks (e.g., report generation, meeting scheduling, data summarization).
â€¢	Facilitated enterprise adoption of agentic chatbot frameworks, leveraging LangChain, semantic memory, and planner-executor architecture to scale LLM automation safely.
â€¢	Built an intermediate middleware layer using Node.js and Azure Functions to transform and relay API data between the Copilot and backend systems, ensuring compatibility and performance.
â€¢	Designed, developed, and deployed end-to-end machine learning solutions integrating advanced forecasting, classification, clustering, regression analysis, and recommendation systems using Databricks, Azure ML, and PySpark.
â€¢	Ensured secure API handling via OAuth 2.0 and JWT-based token authentication for enterprise-grade data access in chatbot workflows.
â€¢	Customized prebuilt AI models to suit organizational tone and response style, aligning Copilot communication with enterprise branding and support guidelines.
â€¢	Developed rich UI experiences using adaptive cards, allowing bots to display structured information like approval buttons, tables, and status dashboards inside Teams chat
â€¢	Built, automated, and maintained large-scale machine learning pipelines for healthcare claims analysis, fraud detection, and member experience optimization, ensuring production stability and compliance.
â€¢	Created an abstraction layer using Azure API Management (APIM) to orchestrate and secure API calls within chatbot flows, ensuring scalability, auditability, and RBAC (Role-Based Access Control) for enterprise use cases.
â€¢	Integrated and pre-processed large datasets from diverse data sources using PySpark and SQL, performing data extraction, cleansing, feature engineering, and transformation to build robust ML models.
â€¢	Developed and deployed machine learning models including Random Forest, Logistic Regression, and Gradient Boosting for patient readmissions prediction, early disease detection, and fraud risk scoring.
â€¢	Structured and organized Azure Data Lake file systems for efficient data storage and retrieval, optimizing data flow for large-scale healthcare analytics.
â€¢	Designed AI agents with recursive task-planning inside chatbots, integrating action-based flows using prompt engineering, retrieval augmentation, and observation-evaluation loops.
â€¢	Created and maintained ER diagrams and data models to support scalable machine learning systems and ensure data integrity across multiple environments.
â€¢	Integrated GraphQL APIs into chatbot flows for querying specific fields and minimizing payload size, optimizing response times and improving efficiency.
â€¢	Established automated model monitoring systems using MLflow and Power BI dashboards to detect model drift, data drift, and performance deviations in real time.
â€¢	Configured channel integrations so that copilots could operate across Microsoft Teams, web chat, and mobile, ensuring consistent and accessible user experiences.
â€¢	Enabled user authentication using Azure Active Directory (AAD) to ensure that Copilot conversations remained secure, user-specific, and compliant with role-based access policies.
â€¢	Deployed models into production using Databricks MLOps practices, ensuring end-to-end reproducibility, version control, and model performance tracking.
â€¢	Developed fallback and error-handling logic in chatbot workflows to gracefully manage API timeouts, retries, and rate limits, preserving user experience in high-concurrency scenarios.
â€¢	Collaborated with cross-functional teams to communicate data insights, recommend actionable strategies, and integrate machine learning solutions into business workflows.
â€¢	Used GitHub and Jenkins for CI/CD (DevOps operations) to automate model deployment. Also familiar with Tortoise SVN, Bitbucket, JIRA, and Confluence for project management. 
â€¢	Extensively involved in all phases of data acquisition, data collection, data cleaning, model development, model validation, and visualization to deliver AI-driven healthcare solutions.


Client: Northern trust, Chicago, IL                                                                                                                  Jan 2023 to Dec 2023
Role: Senior ML Engineer
Roles & Responsibilities:   
â€¢	Led the design and deployment of Generative AI solutions utilizing GPT-4, Azure OpenAI, and Azure Cognitive Search, focusing on enterprise document processing, chatbots, and customer advisory systems.
â€¢	Fine-tuned large language models (LLMs) on domain-specific data, integrating safeguards including prompt engineering, content filtering, and bias mitigation for compliance-sensitive financial applications.
â€¢	Developed modular NLP pipelines using Python, TensorFlow, Keras, NLTK, and SpaCy for text classification, topic modeling, sentiment analysis, and named entity recognition (NER).
â€¢	Implemented robust OAuth 2.0 authentication and access token handling to secure all external API calls made by the chatbot and ensure compliance with enterprise security standards.
â€¢	Designed error-handling patterns within chatbot workflows to gracefully manage failed API calls, network errors, and data mismatches, including retry logic and user notification.
â€¢	Built MLOps frameworks leveraging MLflow and Azure ML for model experimentation, model versioning, and automated deployment, ensuring traceability and reproducibility.
â€¢	Utilized Dataverse to manage and retrieve user-specific and contextual data dynamically within Copilot Studio bots, enhancing personalization and relevance in responses.
â€¢	Built advanced multi-turn dialogues using Power Virtual Agentsâ€™ visual authoring canvas, enabling intuitive end-user conversations that mimic human-like logic and contextual follow-ups.
â€¢	Created and deployed conversational topics using Natural Language Understanding (NLU) to trigger precise actions and fetch data on-the-fly through backend systems.
â€¢	Implemented real-time monitoring of model drift and data drift using automated alerts and dashboards, safeguarding model reliability in live production environments.
â€¢	Architected cloud-native solutions integrating LLM applications with Azure SQL, Neo4j graph databases, and vector search engines for contextual knowledge retrieval and graph-based recommendations.
â€¢	Created and managed a vocabulary using bag-of-words and TF-IDF techniques to encode variables into a machine-readable format.
â€¢	Designed and executed machine learning workflows in a distributed TensorFlow environment, leveraging both CPUs and GPUs for enhanced performance.
â€¢	Enabled asynchronous API call handling with callback mechanisms to ensure long-running transactions (e.g., report generation, email triggering) did not block the user interaction flow.
â€¢	Connected to real-time event streams and webhooks from third-party services to allow chatbots to proactively notify users of key changes, approvals, or alerts.
â€¢	Applied principal component analysis (PCA) and linear discriminant analysis (LDA) to reduce dimensions and improve model efficiency in handling high-dimensional data.
â€¢	Employed various sampling techniques to optimize datasets for balanced learning and inference accuracy.
â€¢	Classified text data within large datasets by implementing NLP categorization, tagging texts with predefined labels using advanced machine learning models.
â€¢	Engineered prompts for LLM applications across customer service automation, code generation, and internal knowledge retrieval systems.
â€¢	Integrated GenAI models with RAG frameworks using vector databases like FAISS and Pinecone to enable contextual and real-time information retrieval.
â€¢	Implemented conversational testing procedures within Copilot Studio to ensure robust performance across scenarios before deployment to end users.
â€¢	Developed and deployed machine learning models using Python and R, leveraging libraries such as Scikit-learn, TensorFlow, and XGBoost for predictive analytics and anomaly detection.
â€¢	Applied a deep understanding of supervised learning algorithms, including linear/logistic regression, decision trees, random forests, and support vector machines (SVM), to solve complex business problems.



**Client: Freedom Mortgage, Boca Raton, FL                                                                                              Oct 2021 to Dec 2022                  Role: Data Scientist/ML Engineer                                                                                                                                             Roles & Responsibilities:**
â€¢	Designed and validated credit risk scoring models using logistic regression and decision trees that aligned with Basel III regulatory requirements.
â€¢	Automated ingestion workflows to extract transactional and market data from SQL Server and MongoDB into a centralized data lake.
â€¢	Built time-series forecasting models for liquidity and cash flow projections under varying macroeconomic scenarios, informing treasury decisions.
â€¢	Developed unsupervised anomaly detection algorithms leveraging isolation forests and autoencoders to flag suspicious trading activities.
â€¢	Constructed feature stores to standardize risk-related variablesâ€”such as loan-to-value ratios and credit utilizationâ€”across multiple model frameworks.
â€¢	Integrated Python analytics modules into Tableau dashboards, enabling real-time visual monitoring of risk metrics by senior management.
â€¢	Collaborated with trading desks to refine volatility forecasts using GARCH models, enhancing market risk management strategies.
â€¢	Conducted scenario analysis and stress tests in accordance with regulatory stress testing guidelines, quantifying potential losses under extreme events.
â€¢	Applied LIME to interpret model outputs, providing transparent explanations for regulators and auditors.
â€¢	Deployed models to Azure ML, leveraging managed endpoints for secure, scalable inference workflows.
â€¢	Established Git-based version control and branching strategies for model code, enabling collaborative development and reproducibility.
â€¢	Optimized complex SQL queries and data joins to reduce ETL runtime by 25%, improving data availability for risk reporting.
â€¢	Created interactive dashboards in Power BI to track model accuracy, population stability, and key performance indicators.
â€¢	Led code review sessions to ensure adherence to coding standards, modular design, and documentation best practices.
â€¢	Implemented Great Expectations for automated data quality validation, catching schema changes and null value spikes early.
â€¢	Researched and tested ensemble techniquesâ€”such as stacking and blendingâ€”to combine model strengths and boost predictive performance.
â€¢	Conducted internal training sessions on advanced ML methods, fostering a data-driven culture within the risk analytics team.


Client: Lowe's, Mooresville, NC                                                                                                         May 2020 to Sep 2021
Role:  ML Engineer
      Roles & Responsibilities:
â€¢	Developed customer segmentation models using k-means and hierarchical clustering on purchase behavior data to inform targeted marketing strategies.
â€¢	Engineered collaborative filtering recommendation systems that delivered personalized vehicle suggestions, increasing click-through rates by 20%.
â€¢	Built NLP pipelines with spaCy and BERT to process and analyze unstructured customer reviews, extracting sentiment and key topic themes.
â€¢	Implemented Apache NiFi workflows to automate ingestion of daily inventory feeds, web logs, and third-party market data.
â€¢	Designed real-time streaming architectures with Kafka and Spark Streaming to track and respond to inventory changes instantly.
â€¢	Trained and fine-tuned convolutional neural networks (ResNet architectures) for automatic vehicle image classification and damage detection.
â€¢	Packaged inference APIs using Flask and Docker, enabling integration of ML services with web and mobile front ends.
â€¢	Orchestrated Kubernetes deployments on AWS EKS to scale model serving pods based on user request volume.
â€¢	Integrated predictive models into Salesforce CRM to deliver dynamic marketing content and drive upsell opportunities.
â€¢	Performed cohort analysis to evaluate marketing campaign effectiveness, using lift charts and transition matrices for decision support.
â€¢	Utilized pandas, scikit-learn, and TensorFlow libraries to preprocess data, train models, and evaluate performance metrics.
â€¢	Conducted rigorous A/B tests to compare recommendation algorithms, selecting optimal models based on statistical significance.
â€¢	Developed Looker dashboards to visualize sales trends, model predictions, and inventory turnover for non-technical stakeholders.
â€¢	Collaborated with data engineers to optimize Redshift schemas and ETL jobs, reducing data latency by 35%.
â€¢	Automated nightly data validation and model retraining via Airflow DAGs, ensuring production models remained current.

**Client: Juspay Technologies, Bengaluru, India                                                                           Jun 2017 to Dec 2019
Role: Data Scientist
Roles& Responsibilities:** 
â€¢	Performed exploratory data analysis on heterogeneous client datasets to identify key variables, trends, and correlations informing project scope.
â€¢	Developed and validated binary classification models using logistic regression and random forests to predict customer churn risk.
â€¢	Executed thorough data cleaning and preprocessing routinesâ€”handling missing values, outliers, and feature scalingâ€”using pandas and NumPy.
â€¢	Engineered domain-specific features such as tenure buckets and usage frequency metrics that enhanced model discriminative power.
â€¢	Assessed model performance with precision, recall, F1-score, and ROC-AUC metrics, iterating on modeling approach to reach business targets.
â€¢	Automated generation of analytical reports and visualizations using Jupyter notebooks, nbconvert, and Puppeteer for stakeholder review.
â€¢	Collaborated with business analysts and product owners to refine requirements, define success criteria, and translate insights into action plans.
â€¢	Implemented unsupervised clustering with k-means to segment customers based on behavior, enabling personalized outreach campaigns.
â€¢	Visualized key insights with matplotlib and seaborn, creating publication-quality charts for presentations and documentation.
â€¢	Authored and maintained comprehensive data dictionaries and ETL workflow documentation to support team onboarding and knowledge transfer.
â€¢	Conducted robust model validation using k-fold cross-validation and bootstrapping methods to ensure generalization on unseen data.
â€¢	Assisted in deploying models to staging environments using Flask microservices, enabling end-to-end integration testing.
â€¢	Participated in client demos and meetings to present analytical findings, explain modeling choices, and gather feedback for iteration.
Environment: Logistic Regression, Random Forests, pandas, NumPy, Jupyter Notebooks, nbconvert, Puppeteer, k-means, matplotlib, seaborn, k-fold Cross-Validation, Bootstrapping, Flask, PostgreSQL, SQL, Git.

Client: iGATE Global Solutions Limited, Ahmedabad, India                                                         Aug 2015 to May 2017
Role: Junior Data Scientist 
       Roles & Responsibilities:
â€¢	Collected, consolidated, and cleansed sales and marketing datasets from ERP systems and Excel spreadsheets to create a unified analysis source.
â€¢	Performed descriptive statistical analysesâ€”mean, median, standard deviationâ€”to summarize key metrics and identify distribution anomalies.
â€¢	Designed and developed interactive dashboards in Tableau to provide real-time visibility into sales performance for regional managers.
â€¢	Automated recurring Excel reports using VBA macros, reducing manual report generation time by 50% and minimizing errors.
â€¢	Assisted in the design and normalization of SQL database schemas to support a centralized data warehouse architecture.
â€¢	Conducted data reconciliation and validation tasks to ensure consistency between source systems and reporting outputs.
â€¢	Documented data collection methodologies, transformation logic, and dashboard user guides for stakeholder reference.
â€¢	Supported senior data scientists and analysts in feature extraction, exploratory analysis, and preliminary model development.


## **Projects**  

### **[Book Recommendation System](https://github.com/Dakshayeni/book-recommendation)**
**Tech Stack:** Python, Scikit-learn, Data Mining, KNN  
- Built a **collaborative filtering recommendation system** using **K-Nearest Neighbors (KNN)**.  
- Improved accuracy through **data preprocessing and cosine similarity metrics**.

### **[Interest Rate Prediction using RNN and LSTM](https://github.com/Dakshayeni/Interest-Rate-Prediction-using-RNN-and-LSTM)**
- Developed a deep learning model using RNN and LSTM to predict interest rates based on historical economic data.
- Preprocessed and normalized macroeconomic indicators, including the Federal Funds Rate and unemployment rates, to improve model accuracy.
- Implemented time-series forecasting techniques to analyze temporal dependencies and long-term trends in financial data.
- Optimized model performance using hyperparameter tuning and evaluated accuracy using Mean Squared Error (MSE).
- Visualized predicted vs. actual interest rates to interpret model performance and derive economic insights.
- Demonstrated expertise in deep learning, time-series analysis, and financial forecasting using Python and TensorFlow/Keras.

### **[Image classification CNN](https://github.com/Dakshayeni/Image-classification-CNN)** 
- Developed a Convolutional Neural Network (CNN) model to classify images of cats and dogs using the Kaggle "Dogs vs. Cats" dataset.**  
- Implemented data preprocessing techniques, including dataset splitting, normalization, and augmentation, to improve model generalization.  
- Experimented with five different CNN architectures to optimize classification accuracy and reduce overfitting.  
- Applied dropout regularization and data augmentation techniques to enhance model robustness and prevent overfitting.  
- Achieved high classification accuracy by fine-tuning hyperparameters and monitoring training/validation loss curves.  


### **[Analytics in Retail Sales: A Machine Learning Approach to Enhancing Customer Experience](https://github.com/Dakshayeni/MachineLearning-Walmart-Retail-Analysis)**	
- Developed a machine learning solution to enhance predictive analytics for retail sales forecasting.
- Utilized algorithms such as Linear Regression, K-Means clustering, and Artificial Neural Networks (ANNs) to predict sales trends and customer purchase behaviors.
- Preprocessed a large dataset (~1.9M records) involving product descriptions, user transactions, and store location data from a multi-national retail chain.
- Improved the accuracy of sales predictions by fine-tuning models and applying advanced feature engineering techniques.
- Created insightful visualizations, including monthly sales patterns, customer segmentation, and geographical sales trends, helping retail managers in strategic decision-making.
- Applied clustering techniques to identify customer groups, which led to targeted marketing campaigns and optimized product bundling.


### **[Predictive Modeling of Chicago Crime Patterns](https://github.com/Dakshayeni/chicago-crime)**
 **Tech Stack:** Python, SQL, Tableau, PySpark, GCP  
- Built **crime prediction models** using **XGBoost, Decision Trees, Random Forest, Naive Bayes** (up to **97% accuracy**).
- Conducted **performance analysis (1-4 worker nodes on GCP)** to optimize execution time.  

### **[NYC Vehicle Collision Analysis](https://github.com/Dakshayeni/nyc-collision-analysis)**  
 **Tech Stack:** Python, Pandas, Tableau  
- Conducted **statistical analysis (Chi-square, ANOVA)** on **New York vehicle collisions**.  
- Developed **interactive Tableau dashboards** for accident trend visualization.    
  

---

## **Skills**  

â€¢	Programming Languages & Libraries : Python , R , SQL, PySpark, Shell Scripting, PowerShell
â€¢	Data Processing & ETL : Apache Airflow, Apache NiFi, Talend Open Studio, SQL, NoSQL
â€¢	Machine Learning Frameworks : TensorFlow, Keras, PyTorch, scikit-learn, XGBoost, LightGBM
â€¢	Big Data Technologies : Hadoop, Spark, Hive, HDFS, Kafka
â€¢	Cloud Platforms : AWS (EC2,RDS,SNS,S3, EMR, Lambda, EKS), Azure (ML, Data Factory), GCP (BigQuery)
â€¢	Data Visualization & Reporting : Tableau, Power BI, Looker, matplotlib, seaborn
â€¢	DevOps & MLOps : Docker, Kubernetes, Jenkins, GitLab CI/CD, Kubeflow
â€¢	Databases : MySQL, PostgreSQL, MongoDB, SQL Server
â€¢	Tools & Environments : Jupyter, GitHub, GitLab, VS Code, JIRA
â€¢	Data Science Platforms: Databricks, Azure Machine Learning, Azure Data Lake
â€¢	Data Modeling: ER Diagrams, Schema Design, Data Lakes Structuring
  

---

## **Contact**  
ðŸ“§ **Email:** [dakshayenibujunuru@gmail.com](mailto:dakshayenibujunuru@gmail.com)  
ðŸ”— **LinkedIn:** [https://www.linkedin.com/in/dakshayeni-bujunuru-84b189143/]([https://linkedin.com/in/dakshayeni/](https://www.linkedin.com/in/dakshayeni-bujunuru-84b189143/))  

---

